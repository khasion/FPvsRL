\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Theoretical Background}{2}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Repeated \& Zero-Sum Stochastic Games}{2}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Fictitious Play (FP)}{2}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Reinforcement Learning (Q-Learning)}{2}{subsection.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.1}Q-Learning with $\epsilon $-Decay}{2}{subsubsection.1.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Modified Games Implementation}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Stochastic Rock-Paper-Scissors}{2}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Zero-Sum Prisoner's Dilemma}{2}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Experimental Results Analysis}{2}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Strategy Evolution}{2}{subsection.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces FP strategy convergence in RPS (Nash equilibrium at 33\% each action). Early oscillations reflect adaptation to QL's exploration phase.\relax }}{2}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:fp_strat}{{1}{2}{FP strategy convergence in RPS (Nash equilibrium at 33\% each action). Early oscillations reflect adaptation to QL's exploration phase.\relax }{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces QL action selection in RPS showing $\epsilon $-decay effects: initial exploration (0-500 episodes) followed by strategy specialization.\relax }}{3}{figure.caption.3}\protected@file@percent }
\newlabel{fig:ql_strat}{{2}{3}{QL action selection in RPS showing $\epsilon $-decay effects: initial exploration (0-500 episodes) followed by strategy specialization.\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces PD behavior: FP's increasing defect probability vs QL's preference for defection (Q1-Q0 $>$ 0). Mutual defection emerges as dominant strategy.\relax }}{3}{figure.caption.4}\protected@file@percent }
\newlabel{fig:pd_behavior}{{3}{3}{PD behavior: FP's increasing defect probability vs QL's preference for defection (Q1-Q0 $>$ 0). Mutual defection emerges as dominant strategy.\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces PD reward divergence: QL's exploitation of defection strategy yields 18\% higher average rewards than FP.\relax }}{3}{figure.caption.5}\protected@file@percent }
\newlabel{fig:pd_rewards}{{4}{3}{PD reward divergence: QL's exploitation of defection strategy yields 18\% higher average rewards than FP.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Cumulative Performance}{4}{subsection.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces RPS cumulative scores: QL maintains $\sim $4\% advantage through mid-game ($\Delta =82$ points at episode 1500).\relax }}{4}{figure.caption.6}\protected@file@percent }
\newlabel{fig:rps_cum}{{5}{4}{RPS cumulative scores: QL maintains $\sim $4\% advantage through mid-game ($\Delta =82$ points at episode 1500).\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces PD cumulative rewards: QL's strategy yields 23.7\% higher cumulative reward than FP by episode 2000.\relax }}{4}{figure.caption.7}\protected@file@percent }
\newlabel{fig:pd_cum}{{6}{4}{PD cumulative rewards: QL's strategy yields 23.7\% higher cumulative reward than FP by episode 2000.\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Conclusion}{4}{section.4}\protected@file@percent }
\gdef \@abspage@last{4}
